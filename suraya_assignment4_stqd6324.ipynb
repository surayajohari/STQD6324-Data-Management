{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jmbx0fZ1xP-sfL7OiaaL8hySJrmDzqje",
      "authorship_tag": "ABX9TyOwcKHd1p/1G34oM/zxA7Zr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surayajohari/STQD6324-Data-Management/blob/main/suraya_assignment4_stqd6324.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NAME**: SURAYA ADNIN BINTI JOHARI\n",
        "\n",
        "**NO. MATRIKS**: P137383\n",
        "\n",
        "**SUBJECT CODE**: STQD6324\n",
        "\n",
        "**SUBJECT NAME**: DATA MANAGEMENT\n",
        "\n",
        "**ASSIGNMENT 4**: Cassandra, HBase, MongoDB"
      ],
      "metadata": {
        "id": "Yu-TXc2mkTx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D4ySLpKjmJJ",
        "outputId": "f229279d-dc6b-42b9-efc4-7e43b6f8d518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. To import Relevant libraries"
      ],
      "metadata": {
        "id": "84l2Hd0qevkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import relevant libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count\n",
        "from cassandra.cluster import Cluster"
      ],
      "metadata": {
        "id": "zVsXRKFqeqcZ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Initialize Spark Session & Cassandra cluster"
      ],
      "metadata": {
        "id": "mzelnGytezlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MovieLensAnalysis\") \\\n",
        "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1:9042\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cassandra keyspace and table names\n",
        "KEYSPACE = \"movielens\"\n",
        "USER_TABLE = \"users\"\n",
        "MOVIE_TABLE = \"movies\"\n",
        "RATING_TABLE = \"ratings\"\n",
        "\n",
        "# Initialize Cassandra Cluster\n",
        "cluster = Cluster(['127.0.0.1:9042'])\n",
        "session = cluster.connect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "6E96oAUhe7RP",
        "outputId": "5961b4c0-e9be-4a4d-e6d7-8583a965c30d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnresolvableContactPoints",
          "evalue": "{}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnresolvableContactPoints\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-87be4d2cf9ed>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Initialize Cassandra Cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'127.0.0.1:9042'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cassandra/cluster.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mcassandra.cluster.Cluster.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnresolvableContactPoints\u001b[0m: {}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the local u.user file and upload it to HDFS\n",
        "def parse_and_upload_to_hdfs(local_file_path, hdfs_path):\n",
        "    import shutil\n",
        "    shutil.copy(local_file_path, hdfs_path)\n",
        "    print(f\"File {local_file_path} uploaded to HDFS path {hdfs_path}\")\n",
        "\n",
        "# Load RDD from HDFS\n",
        "def load_rdd_from_hdfs(file_path):\n",
        "    return spark.sparkContext.textFile(file_path)\n",
        "\n",
        "# Convert RDD to Dataframe\n",
        "def rdd_to_dataframe(rdd, schema):\n",
        "    return spark.createDataFrame(rdd.map(lambda x: x.split('|')).map(lambda x: (x[0], x[1], x[2], x[3])), schema)\n",
        "\n",
        "# Write DataFrame to Cassandra.\n",
        "def write_dataframe_to_cassandra(df, table_name):\n",
        "    df.write.format(\"org.apache.spark.sql.cassandra\").options(table=table_name, keyspace=KEYSPACE).mode(\"append\").save()\n",
        "\n",
        "# Read DataFrame from Cassandra table\n",
        "def read_from_cassandra(table_name):\n",
        "    return spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=table_name, keyspace=KEYSPACE).load()\n",
        "\n",
        "# Calculate average rating for each movie\n",
        "def calculate_avg_rating_per_movie():\n",
        "    ratings_df = read_from_cassandra(RATING_TABLE)\n",
        "\n",
        "    avg_ratings_df = ratings_df.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
        "    avg_ratings_df.show(10)\n",
        "\n",
        "# Identify top ten movies with the highest average ratings.\n",
        "def top_ten_movies_by_avg_rating():\n",
        "    ratings_df = read_from_cassandra(RATING_TABLE)\n",
        "\n",
        "    avg_ratings_df = ratings_df.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
        "    top_ten_movies_df = avg_ratings_df.orderBy(col(\"avg_rating\").desc()).limit(10)\n",
        "    top_ten_movies_df.show()\n",
        "\n",
        "# Find users who have rated at least 50 movies and identify their favourite movie genres.\n",
        "def find_users_with_50_ratings():\n",
        "    ratings_df = read_from_cassandra(RATING_TABLE)\n",
        "    users_df = read_from_cassandra(USER_TABLE)\n",
        "\n",
        "    user_ratings_count_df = ratings_df.groupBy(\"userId\").agg(count(\"movieId\").alias(\"rating_count\"))\n",
        "    active_users_df = user_ratings_count_df.filter(col(\"rating_count\") >= 50)\n",
        "\n",
        "    # Join with user data to find favourite genres (assuming genres are in USER_TABLE)\n",
        "    user_genres_df = active_users_df.join(users_df, \"userId\")\n",
        "    user_genres_df.show()\n",
        "\n",
        "# Find all users with age less than 20 years old.\n",
        "def find_users_under_20():\n",
        "    users_df = read_from_cassandra(USER_TABLE)\n",
        "    young_users_df = users_df.filter(col(\"age\") < 20)\n",
        "    young_users_df.show()\n",
        "\n",
        "# Find all users with occupation \"scientist\" and age between 30 and 40 years old.\n",
        "def find_scientists_between_30_40():\n",
        "    users_df = read_from_cassandra(USER_TABLE)\n",
        "    scientists_df = users_df.filter((col(\"occupation\") == \"scientist\") & (col(\"age\") >= 30) & (col(\"age\") <= 40))\n",
        "    scientists_df.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define file paths\n",
        "    local_file_path = \"/content/drive/MyDrive/STQD6324/u.user\"\n",
        "    hdfs_path = \"hdfs:///user/hadoop/u.user\"\n",
        "\n",
        "    # Parse the file and upload to HDFS\n",
        "    parse_and_upload_to_hdfs(local_file_path, hdfs_path)\n",
        "\n",
        "    # Define schemas\n",
        "    user_schema = [\"userId\", \"age\", \"gender\", \"occupation\"]\n",
        "    rating_schema = [\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
        "\n",
        "    # Load RDDs from HDFS\n",
        "    user_rdd = load_rdd_from_hdfs(hdfs_path)\n",
        "    rating_rdd = load_rdd_from_hdfs(\"hdfs:///user/hadoop/ratings.dat\")  # Adjust path as necessary\n",
        "\n",
        "    # Convert RDDs to DataFrames\n",
        "    user_df = rdd_to_dataframe(user_rdd, user_schema)\n",
        "    rating_df = rdd_to_dataframe(rating_rdd, rating_schema)\n",
        "\n",
        "    # Write DataFrames to Cassandra\n",
        "    write_dataframe_to_cassandra(user_df, USER_TABLE)\n",
        "    write_dataframe_to_cassandra(rating_df, RATING_TABLE)\n",
        "\n",
        "    # Execute queries\n",
        "    calculate_avg_rating_per_movie()\n",
        "    top_ten_movies_by_avg_rating()\n",
        "    find_users_with_50_ratings()\n",
        "    find_users_under_20()\n",
        "    find_scientists_between_30_40()\n",
        "\n",
        "    # Stop the SparkSession\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "d-fVtaM5eo8L",
        "outputId": "ce6900f3-0c17-47ab-b219-081d486427c0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'hdfs:///user/hadoop/u.user'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-a7c9e00157b1>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Parse the file and upload to HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mparse_and_upload_to_hdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Define schemas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-a7c9e00157b1>\u001b[0m in \u001b[0;36mparse_and_upload_to_hdfs\u001b[0;34m(local_file_path, hdfs_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_and_upload_to_hdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {local_file_path} uploaded to HDFS path {hdfs_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hdfs:///user/hadoop/u.user'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sudo systemctl status cassandra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "RydUnBJwfLIz",
        "outputId": "513e5ac0-a18b-43b0-ea3d-0e382fbeba88"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-91-75311a629987>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-91-75311a629987>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sudo systemctl status cassandra\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}